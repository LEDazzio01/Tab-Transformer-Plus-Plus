{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9f8efb",
   "metadata": {},
   "source": [
    "# TabTransformer++ for Residual Learning\n",
    "\n",
    "This notebook demonstrates a **residual learning approach** using a TabTransformer architecture. The key idea is:\n",
    "\n",
    "1. Train a simple \"base\" model (Ridge Regression) to make initial predictions\n",
    "2. Train a TabTransformer to predict the **residuals** (errors) of the base model\n",
    "3. Combine: `Final Prediction = Base Prediction + Predicted Residual`\n",
    "\n",
    "This stacking technique often yields better results than either model alone.\n",
    "\n",
    "## Key Components\n",
    "- **Quantile Binning**: Converts continuous features into discrete tokens\n",
    "- **Gated Fusion**: Learns to balance binned tokens with raw scalar values\n",
    "- **EMA (Exponential Moving Average)**: Polyak averaging for more stable predictions\n",
    "- **Isotonic Calibration**: Post-processing to improve residual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55015a3",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Import required libraries and define hyperparameters:\n",
    "\n",
    "- **Feature Engineering**: Number of bins for quantile discretization\n",
    "- **Model Architecture**: Embedding dimensions, attention heads, transformer layers\n",
    "- **Training**: Learning rate, batch size, EMA decay for Polyak averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed45dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "# ================= Configuration =================\n",
    "class Config:\n",
    "    SEED            = 2025\n",
    "    \n",
    "    # --- Feature Engineering ---\n",
    "    NBINS           = 32        # Quantile bins for raw numeric features\n",
    "    NBINS_BASE      = 128       # Finer bins for Base model prediction\n",
    "    NBINS_DT        = 64        # Bins for DeepTables/Tree model prediction\n",
    "    \n",
    "    # --- Model Architecture ---\n",
    "    EMB_DIM         = 64\n",
    "    N_HEADS         = 4\n",
    "    N_LAYERS        = 3\n",
    "    MLP_HID         = 192\n",
    "    DROPOUT         = 0.1\n",
    "    EMB_DROPOUT     = 0.05\n",
    "    TOKENDROP_P     = 0.12      # Feature noise probability\n",
    "    \n",
    "    # --- Training ---\n",
    "    EPOCHS          = 10        # Shortened for demo\n",
    "    BATCH_SIZE      = 1024\n",
    "    LR              = 2e-3\n",
    "    WEIGHT_DECAY    = 1e-5\n",
    "    EMA_DECAY       = 0.995     # Polyak Averaging\n",
    "    DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.SEED)\n",
    "print(f\"Running on {Config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f73a9",
   "metadata": {},
   "source": [
    "## 2. Data Simulation: Building the \"Stack\"\n",
    "\n",
    "This section simulates a real-world stacking scenario:\n",
    "\n",
    "1. **Load Data**: California Housing dataset (predicting median house values)\n",
    "2. **Train Base Models** using K-Fold cross-validation:\n",
    "   - `Ridge` regression → generates `base_pred` (our primary predictions)\n",
    "   - `RandomForest` → generates `dt_pred` (provides additional signal)\n",
    "3. **Calculate Residuals**: `residual = target - base_pred`\n",
    "   - This is what the TabTransformer will learn to predict\n",
    "\n",
    "The out-of-fold (OOF) predictions prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb997d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Simulating Base & DT Models (The 'Stack') ---\n",
      "Generating OOF predictions...\n",
      "Generating Test predictions...\n",
      "Base Model RMSE (Train OOF): 0.8094\n"
     ]
    }
   ],
   "source": [
    "def get_simulated_data():\n",
    "    \"\"\"\n",
    "    1. Loads California Housing.\n",
    "    2. Trains 'Base' (Ridge) and 'DT' (Random Forest) models to generate OOF predictions.\n",
    "    3. Calculates the 'Residual' (Target - Base).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 1. Simulating Base & DT Models (The 'Stack') ---\")\n",
    "    data = fetch_california_housing(as_frame=True)\n",
    "    df = data.frame\n",
    "    target_col = \"MedHouseVal\"\n",
    "    \n",
    "    # Split Holdout Test Set (acts as 'Private LB')\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=Config.SEED)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    # Placeholders\n",
    "    train_df[\"base_pred\"] = 0.0\n",
    "    train_df[\"dt_pred\"] = 0.0\n",
    "    train_df[\"fold\"] = -1\n",
    "    \n",
    "    # Create Folds for OOF generation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n",
    "    \n",
    "    # Models to simulate your stack\n",
    "    model_base = Ridge(alpha=1.0)\n",
    "    model_dt = RandomForestRegressor(n_estimators=20, max_depth=8, n_jobs=-1, random_state=Config.SEED)\n",
    "    \n",
    "    print(\"Generating OOF predictions...\")\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "        X_tr = train_df.loc[tr_idx].drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "        y_tr = train_df.loc[tr_idx, target_col]\n",
    "        X_val = train_df.loc[val_idx].drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "        \n",
    "        # Fit & Predict Base\n",
    "        model_base.fit(X_tr, y_tr)\n",
    "        train_df.loc[val_idx, \"base_pred\"] = model_base.predict(X_val)\n",
    "        \n",
    "        # Fit & Predict DT\n",
    "        model_dt.fit(X_tr, y_tr)\n",
    "        train_df.loc[val_idx, \"dt_pred\"] = model_dt.predict(X_val)\n",
    "        \n",
    "        train_df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "    # Generate Test Preds (Trained on full train)\n",
    "    print(\"Generating Test predictions...\")\n",
    "    X_full = train_df.drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "    y_full = train_df[target_col]\n",
    "    X_test = test_df.drop(columns=[target_col]) # cols must match, will fill pred later\n",
    "    \n",
    "    # Reuse models trained on full data for test\n",
    "    model_base.fit(X_full, y_full)\n",
    "    test_df[\"base_pred\"] = model_base.predict(X_test)\n",
    "    \n",
    "    model_dt.fit(X_full, y_full)\n",
    "    test_df[\"dt_pred\"] = model_dt.predict(X_test)\n",
    "    \n",
    "    # Calculate Residuals (Target - Base)\n",
    "    # The Transformer will try to predict THIS column\n",
    "    train_df[\"residual\"] = train_df[target_col] - train_df[\"base_pred\"]\n",
    "    \n",
    "    # Identify feature columns\n",
    "    features = [c for c in train_df.columns if c not in [target_col, \"base_pred\", \"dt_pred\", \"fold\", \"residual\"]]\n",
    "    \n",
    "    base_rmse = root_mean_squared_error(train_df[target_col], train_df['base_pred'])\n",
    "    print(f\"Base Model RMSE (Train OOF): {base_rmse:.4f}\")\n",
    "    \n",
    "    return train_df, test_df, features\n",
    "\n",
    "# Run simulation\n",
    "train_df, test_df, features = get_simulated_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad730c",
   "metadata": {},
   "source": [
    "## 3. Tabular Tokenizer\n",
    "\n",
    "The `TabularTokenizer` prepares data for the transformer:\n",
    "\n",
    "### Quantile Binning (Discretization)\n",
    "- Converts continuous features into discrete \"tokens\" (like words in NLP)\n",
    "- Uses quantile-based bins so each bin has roughly equal samples\n",
    "- Different bin counts for features (32), base predictions (128), and tree predictions (64)\n",
    "\n",
    "### Z-Score Normalization\n",
    "- Standardizes raw values: `(x - mean) / std`\n",
    "- Preserves the original numeric information alongside tokens\n",
    "\n",
    "This dual representation (tokens + scalars) gives the model both discrete patterns and continuous precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918bb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTokenizer:\n",
    "    \"\"\"\n",
    "    Handles Quantile Binning and Z-Scoring.\n",
    "    Fits on 'train' subset, transforms 'val'/'test'.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.edges = {}\n",
    "        self.stats = {} # (mean, std)\n",
    "        \n",
    "    def _make_edges(self, x, nbins):\n",
    "        x = x[np.isfinite(x)]\n",
    "        if len(x) == 0: return np.array([0.0, 1.0])\n",
    "        qs = np.linspace(0.0, 1.0, nbins+1)\n",
    "        edges = np.unique(np.quantile(x, qs))\n",
    "        if len(edges) < 2: edges = np.array([x.min(), x.max()+1e-6])\n",
    "        return edges\n",
    "\n",
    "    def fit(self, df):\n",
    "        # 1. Numeric Features\n",
    "        for c in self.cols:\n",
    "            self.edges[c] = self._make_edges(df[c].values, Config.NBINS)\n",
    "            self.stats[c]  = (df[c].mean(), df[c].std() + 1e-8)\n",
    "            \n",
    "        # 2. Special Tokens (Base & DT)\n",
    "        self.edges[\"_base_\"] = self._make_edges(df[\"base_pred\"].values, Config.NBINS_BASE)\n",
    "        self.stats[\"_base_\"] = (df[\"base_pred\"].mean(), df[\"base_pred\"].std() + 1e-8)\n",
    "        \n",
    "        self.edges[\"_dt_\"] = self._make_edges(df[\"dt_pred\"].values, Config.NBINS_DT)\n",
    "        self.stats[\"_dt_\"] = (df[\"dt_pred\"].mean(), df[\"dt_pred\"].std() + 1e-8)\n",
    "        \n",
    "        # 3. Target Stats (for Residual scaling)\n",
    "        self.stats[\"_target_\"] = (df[\"residual\"].mean(), df[\"residual\"].std() + 1e-8)\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Returns: Tokens (Int), Values (Float)\n",
    "        N = len(df)\n",
    "        T = len(self.cols) + 2 # cols + base + dt\n",
    "        \n",
    "        toks = np.zeros((N, T), dtype=np.int64)\n",
    "        vals = np.zeros((N, T), dtype=np.float32)\n",
    "        \n",
    "        def _proc(col_name, edge_key, stat_key, out_idx):\n",
    "            v = df[col_name].values\n",
    "            # Digitize (Binning)\n",
    "            idx = np.searchsorted(self.edges[edge_key], v, side=\"right\") - 1\n",
    "            toks[:, out_idx] = np.clip(idx, 0, len(self.edges[edge_key]) - 2)\n",
    "            # Standardize (Z-Score)\n",
    "            mu, sd = self.stats[stat_key]\n",
    "            vals[:, out_idx] = (v - mu) / sd\n",
    "\n",
    "        # Features\n",
    "        for i, c in enumerate(self.cols):\n",
    "            _proc(c, c, c, i)\n",
    "            \n",
    "        # Base & DT\n",
    "        _proc(\"base_pred\", \"_base_\", \"_base_\", T-2)\n",
    "        _proc(\"dt_pred\",   \"_dt_\",   \"_dt_\",   T-1)\n",
    "        \n",
    "        return toks, vals\n",
    "    \n",
    "    def get_vocab_sizes(self):\n",
    "        s = [len(self.edges[c])-1 for c in self.cols]\n",
    "        s.append(len(self.edges[\"_base_\"])-1)\n",
    "        s.append(len(self.edges[\"_dt_\"])-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57ed73",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "### TabTransformerGated\n",
    "\n",
    "The model combines several key innovations:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| **Token Embeddings** | Learns representations for each quantile bin |\n",
    "| **Value MLPs** | Projects raw scalar values to embedding space |\n",
    "| **Learnable Gates** | Sigmoid gates that blend tokens + scalars per feature |\n",
    "| **CLS Token** | Special token that aggregates information for prediction |\n",
    "| **TokenDrop** | Regularization that randomly masks features during training |\n",
    "| **Transformer Encoder** | Self-attention layers that model feature interactions |\n",
    "\n",
    "### Gated Fusion Formula\n",
    "```\n",
    "embedding[i] = token_emb[i] + σ(gate[i]) × value_emb[i]\n",
    "```\n",
    "The model learns how much to rely on discrete vs. continuous representations for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce61385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"Masks tokens during training for regularization.\"\"\"\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1+T, D]\n",
    "        if not self.training or self.p <= 0: return x\n",
    "        mask = (torch.rand(x.shape[0], x.shape[1], 1, device=x.device) > self.p).float()\n",
    "        mask[:, 0, :] = 1.0 # Never drop CLS\n",
    "        return x * mask\n",
    "\n",
    "class PerTokenValMLP(nn.Module):\n",
    "    \"\"\"Small MLP to project scalar value to embedding space.\"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TabTransformerGated(nn.Module):\n",
    "    def __init__(self, vocab_sizes):\n",
    "        super().__init__()\n",
    "        self.num_tokens = len(vocab_sizes)\n",
    "        \n",
    "        # 1. Embeddings (for Bins)\n",
    "        self.embs = nn.ModuleList([nn.Embedding(v+1, Config.EMB_DIM) for v in vocab_sizes])\n",
    "        \n",
    "        # 2. Value MLPs (for Scalars)\n",
    "        self.val_mlps = nn.ModuleList([PerTokenValMLP(Config.EMB_DIM) for _ in vocab_sizes])\n",
    "        \n",
    "        # 3. Learnable Gates\n",
    "        self.gates = nn.ParameterList([nn.Parameter(torch.zeros(1)) for _ in vocab_sizes])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # 4. Transformer Backbone\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, Config.EMB_DIM))\n",
    "        self.emb_dropout = nn.Dropout(Config.EMB_DROPOUT)\n",
    "        self.tokendrop = TokenDrop(Config.TOKENDROP_P)\n",
    "        \n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=Config.EMB_DIM, nhead=Config.N_HEADS, dim_feedforward=Config.EMB_DIM*4,\n",
    "            dropout=Config.DROPOUT, batch_first=True, norm_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=Config.N_LAYERS)\n",
    "        \n",
    "        # 5. Head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(Config.EMB_DIM),\n",
    "            nn.Linear(Config.EMB_DIM, Config.MLP_HID), nn.GELU(), nn.Dropout(Config.DROPOUT),\n",
    "            nn.Linear(Config.MLP_HID, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_tok, x_val):\n",
    "        B = x_tok.shape[0]\n",
    "        \n",
    "        # Gated Fusion Loop\n",
    "        emb_list = []\n",
    "        for i in range(self.num_tokens):\n",
    "            tok_e = self.embs[i](x_tok[:, i])\n",
    "            val_e = self.val_mlps[i](x_val[:, i:i+1])\n",
    "            g = self.sigmoid(self.gates[i])\n",
    "            emb_list.append(tok_e + g * val_e)\n",
    "            \n",
    "        x = torch.stack(emb_list, dim=1) # [B, T, D]\n",
    "        x = self.emb_dropout(x)\n",
    "        \n",
    "        # Append CLS\n",
    "        cls = self.cls_token.expand(B, 1, -1)\n",
    "        x = torch.cat([cls, x], dim=1) # [B, 1+T, D]\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.tokendrop(x)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        return self.head(x[:, 0, :]).squeeze(-1)\n",
    "\n",
    "class TTDataset(Dataset):\n",
    "    def __init__(self, toks, vals, y=None):\n",
    "        self.toks = torch.as_tensor(toks, dtype=torch.long)\n",
    "        self.vals = torch.as_tensor(vals, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32) if y is not None else None\n",
    "    def __len__(self): return len(self.toks)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.toks[i], self.vals[i]), (self.y[i] if self.y is not None else 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a66de7",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "### Cross-Validation Strategy\n",
    "For each of the 5 folds:\n",
    "\n",
    "1. **Leak-Free Tokenization**: Fit tokenizer only on training data\n",
    "2. **Z-Score Targets**: Normalize residuals for stable training\n",
    "3. **Train with EMA**: \n",
    "   - Main model learns via gradient descent\n",
    "   - EMA model maintains exponential moving average of weights (Polyak averaging)\n",
    "   - EMA often generalizes better than the final trained weights\n",
    "\n",
    "### Isotonic Calibration\n",
    "After training, we calibrate predictions using **Isotonic Regression**:\n",
    "- Maps the model's z-scored outputs back to actual residual values\n",
    "- Monotonic transformation that can correct systematic biases\n",
    "- Fitted on validation data, then applied to test predictions\n",
    "\n",
    "### Final Prediction\n",
    "```\n",
    "final_prediction = base_pred + calibrated_residual\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75fcf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Training Residual TabTransformer (5 folds) ---\n",
      "Fold 0 | Residual RMSE: 0.6073\n",
      "Fold 1 | Residual RMSE: 0.9915\n",
      "Fold 2 | Residual RMSE: 0.6077\n",
      "Fold 3 | Residual RMSE: 0.6098\n",
      "Fold 4 | Residual RMSE: 0.6089\n",
      "\n",
      "=============================================\n",
      "FINAL RESULTS SUMMARY\n",
      "=============================================\n",
      "TRAIN (CV) RMSE:\n",
      "  Base Model Only:      0.80939\n",
      "  Base + TT Residual:   0.70200\n",
      "--------------------\n",
      "TEST (Holdout) RMSE:\n",
      "  Base Model Only:      0.73611\n",
      "  Base + TT Residual:   0.59240\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# Storage for results\n",
    "oof_preds = np.zeros(len(train_df))\n",
    "test_preds_accum = np.zeros(len(test_df))\n",
    "\n",
    "folds = sorted(train_df[\"fold\"].unique())\n",
    "print(f\"\\n--- 2. Training Residual TabTransformer ({len(folds)} folds) ---\")\n",
    "\n",
    "for k in folds:\n",
    "    # A. Split & Leak-Free Tokenization\n",
    "    tr_mask = train_df[\"fold\"] != k\n",
    "    va_mask = train_df[\"fold\"] == k\n",
    "    \n",
    "    tokenizer = TabularTokenizer(features)\n",
    "    tokenizer.fit(train_df[tr_mask]) # Fit on TRAIN ONLY\n",
    "    \n",
    "    # Transform\n",
    "    X_tr_tok, X_tr_val = tokenizer.transform(train_df[tr_mask])\n",
    "    X_va_tok, X_va_val = tokenizer.transform(train_df[va_mask])\n",
    "    X_te_tok, X_te_val = tokenizer.transform(test_df)\n",
    "    \n",
    "    # Targets (Z-Scored)\n",
    "    y_mu, y_std = tokenizer.stats[\"_target_\"]\n",
    "    y_tr = (train_df.loc[tr_mask, \"residual\"].values - y_mu) / y_std\n",
    "    y_va_raw = train_df.loc[va_mask, \"residual\"].values \n",
    "    \n",
    "    # B. Dataloaders\n",
    "    dl_tr = DataLoader(TTDataset(X_tr_tok, X_tr_val, y_tr), batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    dl_va = DataLoader(TTDataset(X_va_tok, X_va_val), batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    dl_te = DataLoader(TTDataset(X_te_tok, X_te_val), batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # C. Init Models (Main + EMA)\n",
    "    model = TabTransformerGated(tokenizer.get_vocab_sizes()).to(Config.DEVICE)\n",
    "    ema_model = TabTransformerGated(tokenizer.get_vocab_sizes()).to(Config.DEVICE)\n",
    "    ema_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=1.0)\n",
    "    \n",
    "    # D. Train Loop\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        for (xt, xv), y in dl_tr:\n",
    "            xt, xv, y = xt.to(Config.DEVICE), xv.to(Config.DEVICE), y.to(Config.DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xt, xv)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            # Update EMA\n",
    "            with torch.no_grad():\n",
    "                for p, ema_p in zip(model.parameters(), ema_model.parameters()):\n",
    "                    ema_p.data.mul_(Config.EMA_DECAY).add_(p.data, alpha=1 - Config.EMA_DECAY)\n",
    "    \n",
    "    # E. Evaluation & Isotonic Calibration\n",
    "    ema_model.eval()\n",
    "    \n",
    "    # 1. Predict Validation (Z-space)\n",
    "    preds_z = []\n",
    "    with torch.no_grad():\n",
    "        for (xt, xv), _ in dl_va:\n",
    "            preds_z.append(ema_model(xt.to(Config.DEVICE), xv.to(Config.DEVICE)).cpu().numpy())\n",
    "    preds_z = np.concatenate(preds_z)\n",
    "    \n",
    "    # 2. Calibrate: Map Z-score Preds -> Real Residuals\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(preds_z, y_va_raw) \n",
    "    calib_preds = iso.predict(preds_z)\n",
    "    \n",
    "    oof_preds[va_mask] = calib_preds\n",
    "    rmse = root_mean_squared_error(y_va_raw, calib_preds)\n",
    "    print(f\"Fold {k} | Residual RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # 3. Predict Test (Apply fold's calibration)\n",
    "    preds_te_z = []\n",
    "    with torch.no_grad():\n",
    "        for (xt, xv), _ in dl_te:\n",
    "            preds_te_z.append(ema_model(xt.to(Config.DEVICE), xv.to(Config.DEVICE)).cpu().numpy())\n",
    "    preds_te_z = np.concatenate(preds_te_z)\n",
    "    test_preds_accum += iso.predict(preds_te_z) / len(folds)\n",
    "    \n",
    "    del model, ema_model, opt, dl_tr\n",
    "    if Config.DEVICE == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "# ================= Results =================\n",
    "# Final Prediction = Base Prediction + Predicted Residual\n",
    "final_oof = train_df[\"base_pred\"] + oof_preds\n",
    "final_test = test_df[\"base_pred\"] + test_preds_accum\n",
    "\n",
    "base_cv = root_mean_squared_error(train_df[\"MedHouseVal\"], train_df[\"base_pred\"])\n",
    "tt_cv   = root_mean_squared_error(train_df[\"MedHouseVal\"], final_oof)\n",
    "\n",
    "base_test = root_mean_squared_error(test_df[\"MedHouseVal\"], test_df[\"base_pred\"])\n",
    "tt_test   = root_mean_squared_error(test_df[\"MedHouseVal\"], final_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*45)\n",
    "print(f\"TRAIN (CV) RMSE:\")\n",
    "print(f\"  Base Model Only:      {base_cv:.5f}\")\n",
    "print(f\"  Base + TT Residual:   {tt_cv:.5f}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"TEST (Holdout) RMSE:\")\n",
    "print(f\"  Base Model Only:      {base_test:.5f}\")\n",
    "print(f\"  Base + TT Residual:   {tt_test:.5f}\")\n",
    "print(\"=\"*45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
