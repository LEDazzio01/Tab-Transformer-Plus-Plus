{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9f8efb",
   "metadata": {},
   "source": [
    "# TabTransformer++ for Residual Learning\n",
    "\n",
    "> **Based on the original TabTransformer paper:**\n",
    "> \n",
    "> Huang, X., Khetan, A., Cvitkovic, M., & Karnin, Z. (2020). *TabTransformer: Tabular Data Modeling Using Contextual Embeddings*. arXiv:2012.06678\n",
    "> \n",
    "> üìÑ [arXiv Paper](https://arxiv.org/abs/2012.06678) | üîó [GitHub](https://github.com/lucidrains/tab-transformer-pytorch)\n",
    "\n",
    "This notebook extends the TabTransformer architecture with additional innovations for **residual learning**. The key idea is:\n",
    "\n",
    "1. Train a simple \"base\" model (Ridge Regression) to make initial predictions\n",
    "2. Train a TabTransformer to predict the **residuals** (errors) of the base model\n",
    "3. Combine: `Final Prediction = Base Prediction + Predicted Residual`\n",
    "\n",
    "This stacking technique often yields better results than either model alone.\n",
    "\n",
    "## Key Components\n",
    "- **Quantile Binning**: Converts continuous features into discrete tokens\n",
    "- **Gated Fusion**: Learns to balance binned tokens with raw scalar values *(novel extension)*\n",
    "- **EMA (Exponential Moving Average)**: Polyak averaging for more stable predictions\n",
    "- **Isotonic Calibration**: Post-processing to improve residual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55015a3",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "Import required libraries and define hyperparameters:\n",
    "\n",
    "- **Feature Engineering**: Number of bins for quantile discretization\n",
    "- **Model Architecture**: Embedding dimensions, attention heads, transformer layers\n",
    "- **Training**: Learning rate, batch size, EMA decay for Polyak averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed45dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - TabTransformer++ Hyperparameters\n",
    "# =============================================================================\n",
    "class Config:\n",
    "    SEED            = 2025\n",
    "    \n",
    "    # --- Quantile Binning (Tokenization) ---\n",
    "    NBINS           = 32        # Bins for raw numeric features\n",
    "    NBINS_BASE      = 128       # Finer bins for base model predictions\n",
    "    NBINS_DT        = 64        # Bins for tree model predictions\n",
    "    \n",
    "    # --- TabTransformer++ Architecture ---\n",
    "    EMB_DIM         = 64        # Embedding dimension (d_model)\n",
    "    N_HEADS         = 4         # Multi-head attention heads\n",
    "    N_LAYERS        = 3         # Transformer encoder layers\n",
    "    MLP_HID         = 192       # Prediction head hidden dimension\n",
    "    DROPOUT         = 0.1       # Attention & feedforward dropout\n",
    "    EMB_DROPOUT     = 0.05      # Post-embedding dropout\n",
    "    TOKENDROP_P     = 0.12      # TokenDrop regularization probability\n",
    "    \n",
    "    # --- Training ---\n",
    "    EPOCHS          = 10        # Training epochs (shortened for demo)\n",
    "    BATCH_SIZE      = 1024\n",
    "    LR              = 2e-3      # AdamW learning rate\n",
    "    WEIGHT_DECAY    = 1e-5      # L2 regularization\n",
    "    EMA_DECAY       = 0.995     # Exponential Moving Average (Polyak averaging)\n",
    "    DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "seed_everything(Config.SEED)\n",
    "print(f\"Running on {Config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f73a9",
   "metadata": {},
   "source": [
    "## 2. Data Simulation: Building the \"Stack\"\n",
    "\n",
    "This section simulates a real-world stacking scenario:\n",
    "\n",
    "1. **Load Data**: California Housing dataset (predicting median house values)\n",
    "2. **Train Base Models** using K-Fold cross-validation:\n",
    "   - `Ridge` regression ‚Üí generates `base_pred` (our primary predictions)\n",
    "   - `RandomForest` ‚Üí generates `dt_pred` (provides additional signal)\n",
    "3. **Calculate Residuals**: `residual = target - base_pred`\n",
    "   - This is what the TabTransformer will learn to predict\n",
    "\n",
    "The out-of-fold (OOF) predictions prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb997d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Simulating Base & DT Models (The 'Stack') ---\n",
      "Generating OOF predictions...\n",
      "Generating Test predictions...\n",
      "Base Model RMSE (Train OOF): 0.8094\n"
     ]
    }
   ],
   "source": [
    "def get_simulated_data():\n",
    "    \"\"\"\n",
    "    Simulate a model stacking scenario for residual learning.\n",
    "    \n",
    "    Steps:\n",
    "        1. Load California Housing dataset\n",
    "        2. Train base models (Ridge, RandomForest) with K-Fold CV\n",
    "        3. Generate out-of-fold (OOF) predictions to avoid leakage\n",
    "        4. Calculate residuals: target - base_prediction\n",
    "    \n",
    "    Returns:\n",
    "        train_df: Training data with base_pred, dt_pred, residual columns\n",
    "        test_df: Test data with base_pred, dt_pred columns\n",
    "        features: List of original feature column names\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 1. Simulating Base & DT Models (The 'Stack') ---\")\n",
    "    data = fetch_california_housing(as_frame=True)\n",
    "    df = data.frame\n",
    "    target_col = \"MedHouseVal\"\n",
    "    \n",
    "    # Hold out 20% as test set (simulates private leaderboard)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=Config.SEED)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize prediction columns\n",
    "    train_df[\"base_pred\"] = 0.0\n",
    "    train_df[\"dt_pred\"] = 0.0\n",
    "    train_df[\"fold\"] = -1\n",
    "    \n",
    "    # K-Fold for leak-free OOF predictions\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=Config.SEED)\n",
    "    \n",
    "    # Base models for the stack\n",
    "    model_base = Ridge(alpha=1.0)\n",
    "    model_dt = RandomForestRegressor(n_estimators=20, max_depth=8, n_jobs=-1, random_state=Config.SEED)\n",
    "    \n",
    "    print(\"Generating OOF predictions...\")\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "        X_tr = train_df.loc[tr_idx].drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "        y_tr = train_df.loc[tr_idx, target_col]\n",
    "        X_val = train_df.loc[val_idx].drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "        \n",
    "        # Ridge regression (base model)\n",
    "        model_base.fit(X_tr, y_tr)\n",
    "        train_df.loc[val_idx, \"base_pred\"] = model_base.predict(X_val)\n",
    "        \n",
    "        # Random Forest (tree-based model)\n",
    "        model_dt.fit(X_tr, y_tr)\n",
    "        train_df.loc[val_idx, \"dt_pred\"] = model_dt.predict(X_val)\n",
    "        \n",
    "        train_df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "    # Generate test predictions (trained on full training set)\n",
    "    print(\"Generating Test predictions...\")\n",
    "    X_full = train_df.drop(columns=[target_col, \"base_pred\", \"dt_pred\", \"fold\"])\n",
    "    y_full = train_df[target_col]\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    \n",
    "    model_base.fit(X_full, y_full)\n",
    "    test_df[\"base_pred\"] = model_base.predict(X_test)\n",
    "    \n",
    "    model_dt.fit(X_full, y_full)\n",
    "    test_df[\"dt_pred\"] = model_dt.predict(X_test)\n",
    "    \n",
    "    # Calculate residuals - this is what TabTransformer++ will predict\n",
    "    train_df[\"residual\"] = train_df[target_col] - train_df[\"base_pred\"]\n",
    "    \n",
    "    # Extract original feature names\n",
    "    features = [c for c in train_df.columns if c not in [target_col, \"base_pred\", \"dt_pred\", \"fold\", \"residual\"]]\n",
    "    \n",
    "    base_rmse = root_mean_squared_error(train_df[target_col], train_df['base_pred'])\n",
    "    print(f\"Base Model RMSE (Train OOF): {base_rmse:.4f}\")\n",
    "    \n",
    "    return train_df, test_df, features\n",
    "\n",
    "# Run simulation\n",
    "train_df, test_df, features = get_simulated_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad730c",
   "metadata": {},
   "source": [
    "## 3. Tabular Tokenizer\n",
    "\n",
    "The `TabularTokenizer` prepares data for the transformer:\n",
    "\n",
    "### Quantile Binning (Discretization)\n",
    "- Converts continuous features into discrete \"tokens\" (like words in NLP)\n",
    "- Uses quantile-based bins so each bin has roughly equal samples\n",
    "- Different bin counts for features (32), base predictions (128), and tree predictions (64)\n",
    "\n",
    "### Z-Score Normalization\n",
    "- Standardizes raw values: `(x - mean) / std`\n",
    "- Preserves the original numeric information alongside tokens\n",
    "\n",
    "This dual representation (tokens + scalars) gives the model both discrete patterns and continuous precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTokenizer:\n",
    "    \"\"\"\n",
    "    Dual-representation tokenizer for TabTransformer++.\n",
    "    \n",
    "    Creates two representations per feature:\n",
    "        1. Token IDs: Quantile bin indices (discrete)\n",
    "        2. Scalar values: Z-score normalized (continuous)\n",
    "    \n",
    "    This enables the Gated Fusion mechanism to blend discrete patterns\n",
    "    with continuous precision.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.edges = {}   # Quantile bin edges per feature\n",
    "        self.stats = {}   # (mean, std) for z-scoring\n",
    "        \n",
    "    def _make_edges(self, x, nbins):\n",
    "        \"\"\"Compute quantile-based bin edges.\"\"\"\n",
    "        x = x[np.isfinite(x)]\n",
    "        if len(x) == 0: \n",
    "            return np.array([0.0, 1.0])\n",
    "        qs = np.linspace(0.0, 1.0, nbins + 1)\n",
    "        edges = np.unique(np.quantile(x, qs))\n",
    "        if len(edges) < 2: \n",
    "            edges = np.array([x.min(), x.max() + 1e-6])\n",
    "        return edges\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit tokenizer on training data only (leak-free).\"\"\"\n",
    "        # Original features\n",
    "        for c in self.cols:\n",
    "            self.edges[c] = self._make_edges(df[c].values, Config.NBINS)\n",
    "            self.stats[c] = (df[c].mean(), df[c].std() + 1e-8)\n",
    "            \n",
    "        # Base model predictions (finer bins for precision)\n",
    "        self.edges[\"_base_\"] = self._make_edges(df[\"base_pred\"].values, Config.NBINS_BASE)\n",
    "        self.stats[\"_base_\"] = (df[\"base_pred\"].mean(), df[\"base_pred\"].std() + 1e-8)\n",
    "        \n",
    "        # Tree model predictions\n",
    "        self.edges[\"_dt_\"] = self._make_edges(df[\"dt_pred\"].values, Config.NBINS_DT)\n",
    "        self.stats[\"_dt_\"] = (df[\"dt_pred\"].mean(), df[\"dt_pred\"].std() + 1e-8)\n",
    "        \n",
    "        # Target (residual) statistics for z-scoring\n",
    "        self.stats[\"_target_\"] = (df[\"residual\"].mean(), df[\"residual\"].std() + 1e-8)\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform data to dual representation.\n",
    "        \n",
    "        Returns:\n",
    "            toks: Token IDs [N, T] - discrete bin indices\n",
    "            vals: Scalar values [N, T] - z-score normalized\n",
    "        \"\"\"\n",
    "        N = len(df)\n",
    "        T = len(self.cols) + 2  # features + base_pred + dt_pred\n",
    "        \n",
    "        toks = np.zeros((N, T), dtype=np.int64)\n",
    "        vals = np.zeros((N, T), dtype=np.float32)\n",
    "        \n",
    "        def _process_column(col_name, edge_key, stat_key, out_idx):\n",
    "            v = df[col_name].values\n",
    "            # Discretize: assign to quantile bins\n",
    "            idx = np.searchsorted(self.edges[edge_key], v, side=\"right\") - 1\n",
    "            toks[:, out_idx] = np.clip(idx, 0, len(self.edges[edge_key]) - 2)\n",
    "            # Normalize: z-score standardization\n",
    "            mu, sd = self.stats[stat_key]\n",
    "            vals[:, out_idx] = (v - mu) / sd\n",
    "\n",
    "        # Process original features\n",
    "        for i, c in enumerate(self.cols):\n",
    "            _process_column(c, c, c, i)\n",
    "            \n",
    "        # Process stacked predictions (base & tree models)\n",
    "        _process_column(\"base_pred\", \"_base_\", \"_base_\", T - 2)\n",
    "        _process_column(\"dt_pred\", \"_dt_\", \"_dt_\", T - 1)\n",
    "        \n",
    "        return toks, vals\n",
    "    \n",
    "    def get_vocab_sizes(self):\n",
    "        \"\"\"Get vocabulary size for each feature's embedding layer.\"\"\"\n",
    "        sizes = [len(self.edges[c]) - 1 for c in self.cols]\n",
    "        sizes.append(len(self.edges[\"_base_\"]) - 1)\n",
    "        sizes.append(len(self.edges[\"_dt_\"]) - 1)\n",
    "        return sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57ed73",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "### TabTransformerGated ‚Äî Architectural Innovations\n",
    "\n",
    "This implementation extends the original TabTransformer with several key innovations for tabular data:\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 1: Dual Representation (Tokens + Scalars)\n",
    "\n",
    "Unlike standard transformers that use only discrete tokens, we maintain **both representations**:\n",
    "\n",
    "| Representation | How it's Created | What it Captures |\n",
    "|----------------|------------------|------------------|\n",
    "| **Token Embedding** | Quantile bin ‚Üí learned embedding | Discrete patterns, ordinal relationships |\n",
    "| **Value Embedding** | Raw scalar ‚Üí MLP projection | Precise numeric magnitude |\n",
    "\n",
    "**Why both?** Binning loses precision (e.g., 1.01 and 1.99 might share a bin), but raw scalars lack the pattern-matching power of embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 2: Learnable Gated Fusion\n",
    "\n",
    "Each feature has a **learnable gate** (initialized to 0) that controls the blend:\n",
    "\n",
    "```\n",
    "final_embedding[i] = token_emb[i] + œÉ(gate[i]) √ó value_emb[i]\n",
    "```\n",
    "\n",
    "- `œÉ(gate)` is a sigmoid, so fusion weight is in [0, 1]\n",
    "- **Gate ‚âà 0**: Model relies mostly on discrete token patterns\n",
    "- **Gate ‚âà 1**: Model uses both discrete + continuous equally\n",
    "- Gates are learned per-feature, so the model adapts to each column's characteristics\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 3: Per-Token Value MLPs\n",
    "\n",
    "Instead of a single shared MLP for all features, each feature gets its **own projection network**:\n",
    "\n",
    "```python\n",
    "PerTokenValMLP: Linear(1 ‚Üí 64) ‚Üí GELU ‚Üí Linear(64 ‚Üí 64) ‚Üí LayerNorm\n",
    "```\n",
    "\n",
    "This allows different features to learn different transformations (e.g., log-like for skewed features, linear for normal ones).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 4: TokenDrop Regularization\n",
    "\n",
    "During training, we randomly **zero out** feature embeddings with probability `p=0.12`:\n",
    "\n",
    "```python\n",
    "mask = (random > p)  # per-sample, per-feature\n",
    "mask[:, 0] = 1.0     # Never drop CLS token\n",
    "x = x * mask\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- Forces model to not over-rely on any single feature\n",
    "- Similar to dropout but at the feature level\n",
    "- Improves generalization on tabular data\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 5: CLS Token Aggregation\n",
    "\n",
    "Following BERT's approach, we prepend a special `[CLS]` token:\n",
    "\n",
    "```\n",
    "Input:  [CLS, feat_1, feat_2, ..., feat_n, base_pred, dt_pred]\n",
    "Output: Use CLS embedding for final prediction\n",
    "```\n",
    "\n",
    "The transformer's self-attention allows CLS to attend to all features and learn a global representation.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ Innovation 6: Pre-LayerNorm Transformer\n",
    "\n",
    "We use `norm_first=True` (Pre-LN) instead of Post-LN:\n",
    "\n",
    "```\n",
    "Pre-LN:  x = x + Attention(LayerNorm(x))\n",
    "Post-LN: x = LayerNorm(x + Attention(x))\n",
    "```\n",
    "\n",
    "Pre-LN is more stable for training and doesn't require careful learning rate warmup.\n",
    "\n",
    "---\n",
    "\n",
    "### Full Architecture Summary\n",
    "\n",
    "```\n",
    "Input: (tokens, values) for each of T features\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Per-Feature Processing (for i in T):  ‚îÇ\n",
    "‚îÇ    token_emb = Embedding(token[i])      ‚îÇ\n",
    "‚îÇ    value_emb = MLP(value[i])            ‚îÇ\n",
    "‚îÇ    gate = sigmoid(learnable_param[i])   ‚îÇ\n",
    "‚îÇ    feat[i] = token_emb + gate*value_emb ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "    Embedding Dropout (p=0.05)\n",
    "         ‚Üì\n",
    "    Prepend [CLS] token\n",
    "         ‚Üì\n",
    "    TokenDrop (p=0.12, training only)\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Transformer Encoder (3 layers):        ‚îÇ\n",
    "‚îÇ    - 4 attention heads                  ‚îÇ\n",
    "‚îÇ    - dim=64, feedforward=256            ‚îÇ\n",
    "‚îÇ    - Pre-LayerNorm, GELU activation     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "    Extract [CLS] embedding\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Prediction Head:                       ‚îÇ\n",
    "‚îÇ    LayerNorm ‚Üí Linear(64‚Üí192) ‚Üí GELU    ‚îÇ\n",
    "‚îÇ    ‚Üí Dropout ‚Üí Linear(192‚Üí1)            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "    Output: Predicted Residual (z-scored)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-level dropout regularization (Innovation #4).\n",
    "    \n",
    "    Randomly zeros out feature embeddings during training to prevent\n",
    "    over-reliance on any single feature. CLS token is never dropped.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, 1+T, D] where first token is CLS\n",
    "        if not self.training or self.p <= 0: \n",
    "            return x\n",
    "        mask = (torch.rand(x.shape[0], x.shape[1], 1, device=x.device) > self.p).float()\n",
    "        mask[:, 0, :] = 1.0  # Preserve CLS token\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "class PerTokenValMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Per-feature value projection network (Innovation #3).\n",
    "    \n",
    "    Each feature gets its own MLP to project scalar values to embedding space.\n",
    "    This allows different features to learn different transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TabTransformerGated(nn.Module):\n",
    "    \"\"\"\n",
    "    TabTransformer++ with Gated Fusion.\n",
    "    \n",
    "    Key architectural innovations:\n",
    "        1. Dual representation (tokens + scalars)\n",
    "        2. Learnable per-feature gates for fusion\n",
    "        3. Per-token value MLPs\n",
    "        4. TokenDrop regularization\n",
    "        5. CLS token for aggregation\n",
    "        6. Pre-LayerNorm transformer (norm_first=True)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_sizes):\n",
    "        super().__init__()\n",
    "        self.num_tokens = len(vocab_sizes)\n",
    "        \n",
    "        # Innovation #1: Token embeddings (discrete representation)\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(v + 1, Config.EMB_DIM) for v in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Innovation #3: Per-feature value MLPs (continuous representation)\n",
    "        self.val_mlps = nn.ModuleList([\n",
    "            PerTokenValMLP(Config.EMB_DIM) for _ in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # Innovation #2: Learnable gates for fusion (initialized to 0)\n",
    "        self.gates = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(1)) for _ in vocab_sizes\n",
    "        ])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Innovation #5: CLS token for global aggregation\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, Config.EMB_DIM))\n",
    "        self.emb_dropout = nn.Dropout(Config.EMB_DROPOUT)\n",
    "        \n",
    "        # Innovation #4: TokenDrop regularization\n",
    "        self.tokendrop = TokenDrop(Config.TOKENDROP_P)\n",
    "        \n",
    "        # Innovation #6: Pre-LayerNorm Transformer (stable training)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=Config.EMB_DIM, \n",
    "            nhead=Config.N_HEADS, \n",
    "            dim_feedforward=Config.EMB_DIM * 4,\n",
    "            dropout=Config.DROPOUT, \n",
    "            batch_first=True, \n",
    "            norm_first=True,  # Pre-LN for stability\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=Config.N_LAYERS)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(Config.EMB_DIM),\n",
    "            nn.Linear(Config.EMB_DIM, Config.MLP_HID),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(Config.DROPOUT),\n",
    "            nn.Linear(Config.MLP_HID, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_tok, x_val):\n",
    "        B = x_tok.shape[0]\n",
    "        \n",
    "        # Gated Fusion: embedding[i] = token_emb + sigmoid(gate) * value_emb\n",
    "        emb_list = []\n",
    "        for i in range(self.num_tokens):\n",
    "            tok_e = self.embs[i](x_tok[:, i])           # Discrete embedding\n",
    "            val_e = self.val_mlps[i](x_val[:, i:i+1])   # Continuous embedding\n",
    "            g = self.sigmoid(self.gates[i])             # Learnable blend weight\n",
    "            emb_list.append(tok_e + g * val_e)\n",
    "            \n",
    "        x = torch.stack(emb_list, dim=1)  # [B, T, D]\n",
    "        x = self.emb_dropout(x)\n",
    "        \n",
    "        # Prepend CLS token for global aggregation\n",
    "        cls = self.cls_token.expand(B, 1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)  # [B, 1+T, D]\n",
    "        \n",
    "        # Apply TokenDrop and Transformer encoder\n",
    "        x = self.tokendrop(x)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Extract CLS embedding for prediction\n",
    "        return self.head(x[:, 0, :]).squeeze(-1)\n",
    "\n",
    "\n",
    "class TTDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for TabTransformer++ dual-representation data.\"\"\"\n",
    "    def __init__(self, toks, vals, y=None):\n",
    "        self.toks = torch.as_tensor(toks, dtype=torch.long)\n",
    "        self.vals = torch.as_tensor(vals, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32) if y is not None else None\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.toks)\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        return (self.toks[i], self.vals[i]), (self.y[i] if self.y is not None else 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a66de7",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "### Cross-Validation Strategy\n",
    "For each of the 5 folds:\n",
    "\n",
    "1. **Leak-Free Tokenization**: Fit tokenizer only on training data\n",
    "2. **Z-Score Targets**: Normalize residuals for stable training\n",
    "3. **Train with EMA**: \n",
    "   - Main model learns via gradient descent\n",
    "   - EMA model maintains exponential moving average of weights (Polyak averaging)\n",
    "   - EMA often generalizes better than the final trained weights\n",
    "\n",
    "### Isotonic Calibration\n",
    "After training, we calibrate predictions using **Isotonic Regression**:\n",
    "- Maps the model's z-scored outputs back to actual residual values\n",
    "- Monotonic transformation that can correct systematic biases\n",
    "- Fitted on validation data, then applied to test predictions\n",
    "\n",
    "### Final Prediction\n",
    "```\n",
    "final_prediction = base_pred + calibrated_residual\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fcf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Training Residual TabTransformer (5 folds) ---\n",
      "Fold 0 | Residual RMSE: 0.6073\n",
      "Fold 1 | Residual RMSE: 0.9915\n",
      "Fold 2 | Residual RMSE: 0.6077\n",
      "Fold 3 | Residual RMSE: 0.6098\n",
      "Fold 4 | Residual RMSE: 0.6089\n",
      "\n",
      "=============================================\n",
      "FINAL RESULTS SUMMARY\n",
      "=============================================\n",
      "TRAIN (CV) RMSE:\n",
      "  Base Model Only:      0.80939\n",
      "  Base + TT Residual:   0.70200\n",
      "--------------------\n",
      "TEST (Holdout) RMSE:\n",
      "  Base Model Only:      0.73611\n",
      "  Base + TT Residual:   0.59240\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Training Loop with K-Fold Cross-Validation\n",
    "# =============================================================================\n",
    "\n",
    "# Storage for predictions\n",
    "oof_preds = np.zeros(len(train_df))        # Out-of-fold residual predictions\n",
    "test_preds_accum = np.zeros(len(test_df))  # Averaged test predictions\n",
    "\n",
    "folds = sorted(train_df[\"fold\"].unique())\n",
    "print(f\"\\n--- 2. Training TabTransformer++ for Residual Learning ({len(folds)} folds) ---\")\n",
    "\n",
    "for k in folds:\n",
    "    # =========================================================================\n",
    "    # A. Leak-Free Data Preparation\n",
    "    # =========================================================================\n",
    "    tr_mask = train_df[\"fold\"] != k\n",
    "    va_mask = train_df[\"fold\"] == k\n",
    "    \n",
    "    # Fit tokenizer on training fold only (prevents data leakage)\n",
    "    tokenizer = TabularTokenizer(features)\n",
    "    tokenizer.fit(train_df[tr_mask])\n",
    "    \n",
    "    # Transform to dual representation (tokens + scalars)\n",
    "    X_tr_tok, X_tr_val = tokenizer.transform(train_df[tr_mask])\n",
    "    X_va_tok, X_va_val = tokenizer.transform(train_df[va_mask])\n",
    "    X_te_tok, X_te_val = tokenizer.transform(test_df)\n",
    "    \n",
    "    # Z-score normalize targets for stable training\n",
    "    y_mu, y_std = tokenizer.stats[\"_target_\"]\n",
    "    y_tr = (train_df.loc[tr_mask, \"residual\"].values - y_mu) / y_std\n",
    "    y_va_raw = train_df.loc[va_mask, \"residual\"].values\n",
    "    \n",
    "    # =========================================================================\n",
    "    # B. Create DataLoaders\n",
    "    # =========================================================================\n",
    "    dl_tr = DataLoader(TTDataset(X_tr_tok, X_tr_val, y_tr), \n",
    "                       batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    dl_va = DataLoader(TTDataset(X_va_tok, X_va_val), \n",
    "                       batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    dl_te = DataLoader(TTDataset(X_te_tok, X_te_val), \n",
    "                       batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # C. Initialize Models (Main + EMA for Polyak Averaging)\n",
    "    # =========================================================================\n",
    "    model = TabTransformerGated(tokenizer.get_vocab_sizes()).to(Config.DEVICE)\n",
    "    ema_model = TabTransformerGated(tokenizer.get_vocab_sizes()).to(Config.DEVICE)\n",
    "    ema_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=1.0)  # Huber loss for robustness\n",
    "    \n",
    "    # =========================================================================\n",
    "    # D. Training Loop with EMA Updates\n",
    "    # =========================================================================\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        for (xt, xv), y in dl_tr:\n",
    "            xt, xv, y = xt.to(Config.DEVICE), xv.to(Config.DEVICE), y.to(Config.DEVICE)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred = model(xt, xv)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            # Update EMA model (Polyak averaging for better generalization)\n",
    "            with torch.no_grad():\n",
    "                for p, ema_p in zip(model.parameters(), ema_model.parameters()):\n",
    "                    ema_p.data.mul_(Config.EMA_DECAY).add_(p.data, alpha=1 - Config.EMA_DECAY)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # E. Evaluation with Isotonic Calibration\n",
    "    # =========================================================================\n",
    "    ema_model.eval()\n",
    "    \n",
    "    # Predict validation set (in z-score space)\n",
    "    preds_z = []\n",
    "    with torch.no_grad():\n",
    "        for (xt, xv), _ in dl_va:\n",
    "            preds_z.append(ema_model(xt.to(Config.DEVICE), xv.to(Config.DEVICE)).cpu().numpy())\n",
    "    preds_z = np.concatenate(preds_z)\n",
    "    \n",
    "    # Isotonic calibration: map z-scored predictions to actual residuals\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(preds_z, y_va_raw)\n",
    "    calib_preds = iso.predict(preds_z)\n",
    "    \n",
    "    oof_preds[va_mask] = calib_preds\n",
    "    rmse = root_mean_squared_error(y_va_raw, calib_preds)\n",
    "    print(f\"Fold {k} | Residual RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Apply calibration to test predictions\n",
    "    preds_te_z = []\n",
    "    with torch.no_grad():\n",
    "        for (xt, xv), _ in dl_te:\n",
    "            preds_te_z.append(ema_model(xt.to(Config.DEVICE), xv.to(Config.DEVICE)).cpu().numpy())\n",
    "    preds_te_z = np.concatenate(preds_te_z)\n",
    "    test_preds_accum += iso.predict(preds_te_z) / len(folds)  # Average across folds\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, ema_model, opt, dl_tr\n",
    "    if Config.DEVICE == \"cuda\": \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# =============================================================================\n",
    "# Final Results: Base + Predicted Residual\n",
    "# =============================================================================\n",
    "final_oof = train_df[\"base_pred\"] + oof_preds\n",
    "final_test = test_df[\"base_pred\"] + test_preds_accum\n",
    "\n",
    "base_cv = root_mean_squared_error(train_df[\"MedHouseVal\"], train_df[\"base_pred\"])\n",
    "tt_cv = root_mean_squared_error(train_df[\"MedHouseVal\"], final_oof)\n",
    "\n",
    "base_test = root_mean_squared_error(test_df[\"MedHouseVal\"], test_df[\"base_pred\"])\n",
    "tt_test = root_mean_squared_error(test_df[\"MedHouseVal\"], final_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESULTS: TabTransformer++ Residual Learning\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"TRAIN (Cross-Validation) RMSE:\")\n",
    "print(f\"  Base Model Only:           {base_cv:.5f}\")\n",
    "print(f\"  Base + TabTransformer++:   {tt_cv:.5f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"TEST (Holdout) RMSE:\")\n",
    "print(f\"  Base Model Only:           {base_test:.5f}\")\n",
    "print(f\"  Base + TabTransformer++:   {tt_test:.5f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
